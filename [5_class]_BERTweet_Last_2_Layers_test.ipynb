{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[5 class] BERTweet Last 2 Layers test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Q6g8NnOr0uF",
        "outputId": "969ad5a3-d39b-4ae3-aca8-71f96d36ea70"
      },
      "source": [
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yU10P7fm329r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaTr5gWUsWju",
        "outputId": "461b7964-a1ff-4b75-ddfc-b5db4a014fcb"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOJvJDnksXl9",
        "outputId": "59baef45-7e5a-4848-a39e-778ec6fd85c4"
      },
      "source": [
        "%cd transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psYweV9CsaYe",
        "outputId": "f9bf87c7-084a-4f64-96ad-93a8ce9ca30b"
      },
      "source": [
        "!pip3 install --upgrade ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.1.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.13.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.13.0.dev0-py3-none-any.whl size=3179591 sha256=dc78ca06c1e16d1fc43e1e093bdc75eb39b67d05b253efe7b516540dc2ebae34\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5btz51cm/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.13.0.dev0\n",
            "    Uninstalling transformers-4.13.0.dev0:\n",
            "      Successfully uninstalled transformers-4.13.0.dev0\n",
            "Successfully installed transformers-4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iEjSNcRsgft"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import functional as F\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, confusion_matrix\n",
        "from transformers import get_linear_schedule_with_warmup,AdamW,AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import TensorDataset,DataLoader, RandomSampler, SequentialSampler, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kft-cf1Pska-"
      },
      "source": [
        "batch_size = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rLyoyKrsm5t"
      },
      "source": [
        "def calculate_scores(preds, labels):\n",
        "    pred_flat = np.argmax(np.concatenate(preds), axis=1).flatten()\n",
        "    results = dict()\n",
        "    results['precision_score'] = precision_score(labels, pred_flat, average='macro')\n",
        "    results['recall_score'] = recall_score(labels, pred_flat, average='macro')\n",
        "    results['f1_score'] = f1_score(labels, pred_flat, average='macro')\n",
        "    results['confusion_matrix'] = confusion_matrix(labels,pred_flat)\n",
        "    return results\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "582GwErYsogm"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSFeWQ0NsoR2"
      },
      "source": [
        "def convert_label(label):\n",
        "    if label == \"desirability\":\n",
        "        return 0\n",
        "    elif label == \"enabling context\":\n",
        "        return 1\n",
        "    elif label == \"can do\":\n",
        "        return 2\n",
        "    elif label == \"buzz\":\n",
        "        return 3\n",
        "    elif label == \"invitation\":\n",
        "        return 4\n",
        "    else:\n",
        "        raise Exception(\"label classes must be 'desirability' or 'enabling context' or 'buzz' or 'can do' or 'invitation'\")\n",
        "        \n",
        "        \n",
        "def convert_prediction(pred):\n",
        "     if pred == \"desirability\":\n",
        "        return 0\n",
        "     elif pred == \"enabling context\":\n",
        "        return 1\n",
        "     elif pred == \"can do\":\n",
        "        return 2\n",
        "     elif pred == \"buzz\":\n",
        "        return 3\n",
        "     elif pred == \"invitation\":\n",
        "        return 4\n",
        "     else:\n",
        "        raise Exception(\"Prediction classes must be 'desirability' or 'enabling context' or 'buzz' or 'can do' or 'invitation'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Ngwj8Qu1q2",
        "outputId": "8c42491c-f569-49d2-d2c7-c276662c6f76"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "OHtB7Xarss9V",
        "outputId": "473e0569-b885-4205-cd58-bce9812c56c5"
      },
      "source": [
        "df = pd.read_csv('Dataset_V4.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>decision</th>\n",
              "      <th>created_at</th>\n",
              "      <th>id</th>\n",
              "      <th>id_str</th>\n",
              "      <th>full_text</th>\n",
              "      <th>source</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>reply</th>\n",
              "      <th>username_length</th>\n",
              "      <th>user_location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>enabling context</td>\n",
              "      <td>2021-01-30 02:40:25+00:00</td>\n",
              "      <td>1355345090022154241</td>\n",
              "      <td>1355345090022154241</td>\n",
              "      <td>@science_barbie @priyology Cyanobacteria (also...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>enabling context</td>\n",
              "      <td>2021-01-30 01:05:16+00:00</td>\n",
              "      <td>1355321147701686273</td>\n",
              "      <td>1355321147701686273</td>\n",
              "      <td>New insights into coral bleaching by internati...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>10</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>buzz</td>\n",
              "      <td>2021-01-29 22:49:17+00:00</td>\n",
              "      <td>1355286925809094659</td>\n",
              "      <td>1355286925809094659</td>\n",
              "      <td>Friday Tally:\\n\\n2 dives, continued coral blea...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>11</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>desirability</td>\n",
              "      <td>2021-01-29 22:09:20+00:00</td>\n",
              "      <td>1355276872410681348</td>\n",
              "      <td>1355276872410681348</td>\n",
              "      <td>Coral bleaching is so sad because they just be...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>False</td>\n",
              "      <td>8</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>desirability</td>\n",
              "      <td>2021-01-29 21:34:44+00:00</td>\n",
              "      <td>1355268165148815360</td>\n",
              "      <td>1355268165148815360</td>\n",
              "      <td>predicts that at the current rate of emissions...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "      <td>Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1217</th>\n",
              "      <td>1217</td>\n",
              "      <td>buzz</td>\n",
              "      <td>2021-05-17 19:30:12+00:00</td>\n",
              "      <td>1394374712835264513</td>\n",
              "      <td>1394374712835264513</td>\n",
              "      <td>#ClimateChampions developed learning resources...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>False</td>\n",
              "      <td>10</td>\n",
              "      <td>United States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218</th>\n",
              "      <td>1218</td>\n",
              "      <td>desirability</td>\n",
              "      <td>2021-05-17 17:37:33+00:00</td>\n",
              "      <td>1394346363736018949</td>\n",
              "      <td>1394346363736018949</td>\n",
              "      <td>\"For all we hear about #coral bleaching &amp;amp; ...</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>11</td>\n",
              "      <td>24</td>\n",
              "      <td>False</td>\n",
              "      <td>14</td>\n",
              "      <td>Unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219</th>\n",
              "      <td>1219</td>\n",
              "      <td>enabling context</td>\n",
              "      <td>2021-05-17 17:01:39+00:00</td>\n",
              "      <td>1394337327930122250</td>\n",
              "      <td>1394337327930122250</td>\n",
              "      <td>Corals are critical. Track coral bleaching in ...</td>\n",
              "      <td>Hootsuite Inc.</td>\n",
              "      <td>10</td>\n",
              "      <td>29</td>\n",
              "      <td>False</td>\n",
              "      <td>13</td>\n",
              "      <td>United States</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>1220</td>\n",
              "      <td>enabling context</td>\n",
              "      <td>2021-05-17 16:09:02+00:00</td>\n",
              "      <td>1394324089200779268</td>\n",
              "      <td>1394324089200779268</td>\n",
              "      <td>The number one root cause of coral reefs facin...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "      <td>South Korea</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1221</th>\n",
              "      <td>1221</td>\n",
              "      <td>enabling context</td>\n",
              "      <td>2021-05-17 16:08:22+00:00</td>\n",
              "      <td>1394323919453130758</td>\n",
              "      <td>1394323919453130758</td>\n",
              "      <td>99 percent of corals at the #GreatBarrierReef ...</td>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>South Korea</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1222 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0          decision  ... username_length  user_location\n",
              "0              0  enabling context  ...               8        Unknown\n",
              "1              1  enabling context  ...              10        Unknown\n",
              "2              2              buzz  ...              11        Unknown\n",
              "3              3      desirability  ...               8        Unknown\n",
              "4              4      desirability  ...              13         Canada\n",
              "...          ...               ...  ...             ...            ...\n",
              "1217        1217              buzz  ...              10  United States\n",
              "1218        1218      desirability  ...              14        Unknown\n",
              "1219        1219  enabling context  ...              13  United States\n",
              "1220        1220  enabling context  ...              12    South Korea\n",
              "1221        1221  enabling context  ...              12    South Korea\n",
              "\n",
              "[1222 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTTmsMLOtL72"
      },
      "source": [
        "df = df[['decision','full_text']]\n",
        "df = df.rename(columns={'full_text':'text','decision':'label'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVpP8EuSu7vW"
      },
      "source": [
        "df['label'] = df['label'].map({\n",
        "    'desirability': 0, \n",
        "    'enabling context': 1,\n",
        "    'can do': 2,\n",
        "    'buzz':3,\n",
        "    'invitation': 4\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gUtqfY4vBA-"
      },
      "source": [
        "df['label'] = df['label'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-l4A5tqvEC2"
      },
      "source": [
        "Split into TEST and TRAIN\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ7rHCAHvHTF",
        "outputId": "edfa5037-4641-46f4-fdcf-450f9669afec"
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "\n",
        "for train_idx, test_idx in sss.split(df[['text']], df[['label']]):\n",
        "  train = df.iloc[train_idx]\n",
        "  test = df.iloc[test_idx]\n",
        "\n",
        "print(len(train))\n",
        "print(len(test))\n",
        "val = test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "977\n",
            "245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3WN_sUzvJq1"
      },
      "source": [
        "train_df = train.reset_index(drop=True)\n",
        "dev_df = val.reset_index(drop=True)\n",
        "test_df = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "BCbumUmQvKuu",
        "outputId": "ff758dcb-1725-4ed7-f47f-79e076ac356b"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>Thanks to David Vaughan and Chris Page for mak...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>An intense marine heatwave occurred in 2016.\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>We are looking for a new Coral Biologist at Ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Join me and authors @Angie_Hockman and @Holroy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>.@AMV_BBDO and @ShebaBrand's dramatic satellit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>2</td>\n",
              "      <td>Thankfully this year, cyclones saved the coral...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>0</td>\n",
              "      <td>Higher temperature has contributed to coral bl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>0</td>\n",
              "      <td>A reminder that climate change is not happenin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>0</td>\n",
              "      <td>#Drought over the southwestern Tibetan Plateau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>1</td>\n",
              "      <td>Coral reef resilience differs among islands wi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>977 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                               text\n",
              "0        3  Thanks to David Vaughan and Chris Page for mak...\n",
              "1        0  An intense marine heatwave occurred in 2016.\\n...\n",
              "2        4  We are looking for a new Coral Biologist at Ve...\n",
              "3        4  Join me and authors @Angie_Hockman and @Holroy...\n",
              "4        3  .@AMV_BBDO and @ShebaBrand's dramatic satellit...\n",
              "..     ...                                                ...\n",
              "972      2  Thankfully this year, cyclones saved the coral...\n",
              "973      0  Higher temperature has contributed to coral bl...\n",
              "974      0  A reminder that climate change is not happenin...\n",
              "975      0  #Drought over the southwestern Tibetan Plateau...\n",
              "976      1  Coral reef resilience differs among islands wi...\n",
              "\n",
              "[977 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "lAIBkRqIvsOF",
        "outputId": "c9a36322-0235-45c3-8acb-8c93dcfc00ca"
      },
      "source": [
        "dev_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The waters off southern NSW and the east coast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>From coral restoration dives to cleanup dives ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Fairmont Maldives Sirru Fen Fushi announces th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>The legislation bans the sale of over-the-coun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Volunteer pup Mango helping to get the word ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>0</td>\n",
              "      <td>For example, Rising temperatures cause coral b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>1</td>\n",
              "      <td>@wlbeeton @ddasser @ClimateDepot 93% of global...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>3</td>\n",
              "      <td>Think these corals are bleached? Think again! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>1</td>\n",
              "      <td>Coral Species Respond Differently To Ocean War...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0</td>\n",
              "      <td>\"The growth of coral reefs is threatened by th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>245 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                               text\n",
              "0        0  The waters off southern NSW and the east coast...\n",
              "1        2  From coral restoration dives to cleanup dives ...\n",
              "2        3  Fairmont Maldives Sirru Fen Fushi announces th...\n",
              "3        1  The legislation bans the sale of over-the-coun...\n",
              "4        3  Volunteer pup Mango helping to get the word ou...\n",
              "..     ...                                                ...\n",
              "240      0  For example, Rising temperatures cause coral b...\n",
              "241      1  @wlbeeton @ddasser @ClimateDepot 93% of global...\n",
              "242      3  Think these corals are bleached? Think again! ...\n",
              "243      1  Coral Species Respond Differently To Ocean War...\n",
              "244      0  \"The growth of coral reefs is threatened by th...\n",
              "\n",
              "[245 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gX5IW7dwwZPN",
        "outputId": "230c0abe-bbad-4172-da91-0d0ec1238e5c"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The waters off southern NSW and the east coast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>From coral restoration dives to cleanup dives ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Fairmont Maldives Sirru Fen Fushi announces th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>The legislation bans the sale of over-the-coun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Volunteer pup Mango helping to get the word ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>0</td>\n",
              "      <td>For example, Rising temperatures cause coral b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>1</td>\n",
              "      <td>@wlbeeton @ddasser @ClimateDepot 93% of global...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>3</td>\n",
              "      <td>Think these corals are bleached? Think again! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>1</td>\n",
              "      <td>Coral Species Respond Differently To Ocean War...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>0</td>\n",
              "      <td>\"The growth of coral reefs is threatened by th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>245 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                               text\n",
              "0        0  The waters off southern NSW and the east coast...\n",
              "1        2  From coral restoration dives to cleanup dives ...\n",
              "2        3  Fairmont Maldives Sirru Fen Fushi announces th...\n",
              "3        1  The legislation bans the sale of over-the-coun...\n",
              "4        3  Volunteer pup Mango helping to get the word ou...\n",
              "..     ...                                                ...\n",
              "240      0  For example, Rising temperatures cause coral b...\n",
              "241      1  @wlbeeton @ddasser @ClimateDepot 93% of global...\n",
              "242      3  Think these corals are bleached? Think again! ...\n",
              "243      1  Coral Species Respond Differently To Ocean War...\n",
              "244      0  \"The growth of coral reefs is threatened by th...\n",
              "\n",
              "[245 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00tKtZcfwxbV"
      },
      "source": [
        "def bert_encode(df, tokenizer):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sentence in df[[\"text\"]].values:\n",
        "        sentence = sentence.item()\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            sentence,                      \n",
        "                            add_special_tokens = True,  \n",
        "                            max_length = 72,\n",
        "                            pad_to_max_length = True,\n",
        "                            truncation = True,\n",
        "                            return_attention_mask = True,   \n",
        "                            return_tensors = 'pt',    \n",
        "                    )\n",
        "           \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    inputs = {\n",
        "    'input_word_ids': input_ids,\n",
        "    'input_mask': attention_masks}\n",
        "\n",
        "    return inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6idlfOdxc8V"
      },
      "source": [
        "def prepare_dataloaders(train_df,test_df,dev_df, batch_size):\n",
        "    # Load the AutoTokenizer with a normalization mode if the input Tweet is raw\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "\n",
        "    tweet_valid = bert_encode(dev_df, tokenizer)\n",
        "    tweet_valid_labels = dev_df.label.astype(int)\n",
        "    \n",
        "    tweet_train = bert_encode(train_df, tokenizer)\n",
        "    tweet_train_labels = train_df.label.astype(int)\n",
        "    \n",
        "    tweet_test = bert_encode(test_df, tokenizer)\n",
        "\n",
        "\n",
        "    input_ids, attention_masks = tweet_train.values()\n",
        "    labels = torch.tensor(tweet_train_labels.values)\n",
        "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "    \n",
        "    input_ids, attention_masks = tweet_valid.values()\n",
        "    labels = torch.tensor(tweet_valid_labels.values)\n",
        "    val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "    \n",
        "    input_ids, attention_masks = tweet_test.values()\n",
        "    test_dataset = TensorDataset(input_ids, attention_masks)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "                train_dataset,\n",
        "                sampler = RandomSampler(train_dataset), \n",
        "                batch_size = batch_size \n",
        "            )\n",
        "\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "                val_dataset, \n",
        "                sampler = SequentialSampler(val_dataset),\n",
        "                batch_size = batch_size \n",
        "            )\n",
        "\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "                test_dataset, \n",
        "                sampler = SequentialSampler(test_dataset), \n",
        "                batch_size = batch_size\n",
        "            )\n",
        "    \n",
        "    return train_dataloader,validation_dataloader,test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA_E5S9qxe6F",
        "outputId": "5498b7d6-cab2-40eb-f1ad-604a699fb33e"
      },
      "source": [
        "train_dataloader,validation_dataloader,test_dataloader = prepare_dataloaders(train_df, test_df, dev_df, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnCUICfOxipU"
      },
      "source": [
        "Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNwEfbY2xkOV"
      },
      "source": [
        "def test_encode(sentence):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sentence,                      \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 72,\n",
        "                        pad_to_max_length = True,\n",
        "                        truncation = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',    \n",
        "                )\n",
        "           \n",
        "    return encoded_dict['input_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwPSVL8OxnBl"
      },
      "source": [
        "def test_decode(tokens):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
        "    return tokenizer.convert_ids_to_tokens(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qAxOojWTxnvN",
        "outputId": "7edf6953-da8f-4399-e1d0-b459945ef4d6"
      },
      "source": [
        "train_df.text[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Thanks to David Vaughan and Chris Page for making this possible. https://t.co/q4qaW3Lca4'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HORiysbjxp49",
        "outputId": "986d0bc4-feed-4e71-d126-4fe505ba8212"
      },
      "source": [
        "text_test = train_df.text[0]\n",
        "text_encoded = test_encode(text_test)\n",
        "text_decoded = test_decode(text_encoded[0, :130])\n",
        "\n",
        "\n",
        "print(f'Shape      : {text_encoded.shape}')\n",
        "print(f'Word Ids   : {text_encoded}')\n",
        "print(f'Decoded Words   : {text_decoded}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2227: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape      : torch.Size([1, 72])\n",
            "Word Ids   : tensor([[    0,   404,     9,  1593, 42578,    13,  1687,  5810,    19,   382,\n",
            "            33,  1183,     4,    10,     2,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]])\n",
            "Decoded Words   : ['<s>', 'Thanks', 'to', 'David', 'Vaughan', 'and', 'Chris', 'Page', 'for', 'making', 'this', 'possible', '.', 'HTTPURL', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfdzwRCXxu50"
      },
      "source": [
        "Optimizer for BERTweet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqMJxLLAyC09",
        "outputId": "c669d5d9-cdc8-481e-d29c-3768076a2b13"
      },
      "source": [
        "from transformers import RobertaConfig, RobertaModel, BertPreTrainedModel\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch import nn\n",
        "from typing import Tuple\n",
        "import torch\n",
        "\n",
        "!wget https://public.vinai.io/BERTweet_base_transformers.tar.gz\n",
        "\n",
        "!tar -xzvf BERTweet_base_transformers.tar.gz\n",
        "\n",
        "class BERTweetModelForClassification(BertPreTrainedModel):\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.num_labels: int = 5\n",
        "        config: RobertaConfig = RobertaConfig.from_pretrained(\n",
        "            \"vinai/bertweet-base\",\n",
        "            output_hidden_states=True,\n",
        "        )\n",
        "        super().__init__(config)\n",
        "        self.model: RobertaModel = RobertaModel.from_pretrained(\n",
        "            \"vinai/bertweet-base\",\n",
        "            config=config\n",
        "        )\n",
        "        self.dense = nn.Linear(in_features=768 * 2,\n",
        "                               out_features=768,\n",
        "                               )\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.dense_2 = nn.Linear(in_features=768,\n",
        "                                 out_features=256,\n",
        "                                 )\n",
        "        self.classifier = nn.Linear(in_features=256,\n",
        "                                    out_features=self.num_labels,\n",
        "                                    )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        # Take <CLS> token for Native Layer Norm Backward\n",
        "        hidden_states: Tuple[torch.tensor] = outputs[2]\n",
        "        last_sequence_output: torch.tensor = hidden_states[-1][:, 0, :]\n",
        "        second_to_last_sequence_output: torch.tensor = hidden_states[-2][:, 0, :]\n",
        "\n",
        "        sequence_output: torch.tensor = torch.cat((\n",
        "            last_sequence_output,\n",
        "            second_to_last_sequence_output\n",
        "        ), dim=1)\n",
        "\n",
        "        sequence_output = self.dense(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        sequence_output = self.dense_2(sequence_output)\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "\n",
        "        logits: torch.tensor = self.classifier(sequence_output)\n",
        "        outputs = (logits,)\n",
        "        if labels is not None:\n",
        "            loss_function = CrossEntropyLoss()\n",
        "            loss = loss_function(\n",
        "                logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "        return outputs  # loss, logits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-30 06:53:24--  https://public.vinai.io/BERTweet_base_transformers.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 65.8.49.115, 65.8.49.107, 65.8.49.47, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|65.8.49.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 322076118 (307M) [application/x-tar]\n",
            "Saving to: ‘BERTweet_base_transformers.tar.gz.9’\n",
            "\n",
            "BERTweet_base_trans 100%[===================>] 307.16M  23.8MB/s    in 15s     \n",
            "\n",
            "2021-11-30 06:53:40 (20.5 MB/s) - ‘BERTweet_base_transformers.tar.gz.9’ saved [322076118/322076118]\n",
            "\n",
            "BERTweet_base_transformers/\n",
            "BERTweet_base_transformers/config.json\n",
            "BERTweet_base_transformers/bpe.codes\n",
            "BERTweet_base_transformers/model.bin\n",
            "BERTweet_base_transformers/dict.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JfMUZBOym1N",
        "outputId": "8a1cb519-2d19-4470-fbb2-c6ef80deb8f9"
      },
      "source": [
        "model = BERTweetModelForClassification()\n",
        "epochs = 7\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 1e-4,\n",
        "                  eps = 1e-4)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 1,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqsmdbj3y7iU"
      },
      "source": [
        "import sys\n",
        "def validate(model,validation_dataloader, val_labels):\n",
        "    model.eval()\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.to(device)\n",
        "    \n",
        "    preds = []\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    t0 = time.time()\n",
        "    \n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        with torch.no_grad():\n",
        "            loss, logits = model(b_input_ids,\n",
        "                                 attention_mask=b_input_mask,\n",
        "                                 labels=b_labels)\n",
        "\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        preds.append(logits)\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "    \n",
        "    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "    print(\"  Accuracy: {0:.3f} %\".format(avg_val_accuracy*100))\n",
        "    avg_val_loss = total_eval_loss / len(test_dataloader)\n",
        "    print(\"  Test Loss: {0:.3f}\".format(avg_val_loss))\n",
        "    \n",
        "    scores = calculate_scores(preds, val_labels)\n",
        "    print(\"  Confusion Matrix: \\n{:}\".format(scores['confusion_matrix']))\n",
        "    print(\"  Precision Score: {0:.3f} %\".format(scores['precision_score']*100))\n",
        "    print(\"  Recall Score: {0:.3f} %\".format(scores['recall_score']*100))\n",
        "    print(\"  F1 Score: {0:.3f} %\".format(scores['f1_score']*100))\n",
        "\n",
        "    if avg_val_accuracy*100 >= 75 and scores['f1_score']*100 >= 74:\n",
        "      torch.save(model.cpu().state_dict(),\"ModelEE4.pth\")\n",
        "\n",
        "      model = BERTweetModelForClassification()\n",
        "      model.load_state_dict(torch.load(\"ModelEE4.pth\"))\n",
        "      torch.save(model, \"Model_EE4_fix.pth\")\n",
        "\n",
        "      sys.exit(\"Target Acquired. Stopping training process...\")\n",
        "\n",
        "    \n",
        "    return preds, avg_val_accuracy, avg_val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z8alNLczCul"
      },
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, validation_dataloader, val_labels, epochs):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    \n",
        "    model.to(device)\n",
        "    \n",
        "    training_stats = []\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    for epoch_i in range(0, epochs):\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training:')\n",
        "        \n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        model.train()\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "            \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            model.zero_grad() \n",
        "                   \n",
        "            loss, logits = model(b_input_ids,\n",
        "                                 attention_mask=b_input_mask,\n",
        "                                 labels=b_labels)\n",
        "            total_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "            \n",
        "        _, avg_val_accuracy, avg_val_loss = validate(model,validation_dataloader, val_labels)\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xRe9KaluzEBM",
        "outputId": "5d42160c-dfd6-4a59-90dd-93f124e9558e"
      },
      "source": [
        "train(model,optimizer,scheduler,train_dataloader,validation_dataloader, dev_df.label.astype(int), epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 7 ========\n",
            "Training:\n",
            "  Batch    40  of    245.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    245.    Elapsed: 0:00:18.\n",
            "  Batch   120  of    245.    Elapsed: 0:00:26.\n",
            "  Batch   160  of    245.    Elapsed: 0:00:35.\n",
            "  Batch   200  of    245.    Elapsed: 0:00:44.\n",
            "  Batch   240  of    245.    Elapsed: 0:00:53.\n",
            "\n",
            "  Training loss: 1.18\n",
            "  Training epoch took: 0:00:54\n",
            "  Accuracy: 60.081 %\n",
            "  Test Loss: 0.920\n",
            "  Confusion Matrix: \n",
            "[[73  2  0  0  0]\n",
            " [57 28  0  1  0]\n",
            " [ 2  2  0  6  0]\n",
            " [ 4 13  0 45  0]\n",
            " [ 0  1  0 11  0]]\n",
            "  Precision Score: 37.195 %\n",
            "  Recall Score: 40.494 %\n",
            "  F1 Score: 36.724 %\n",
            "\n",
            "======== Epoch 2 / 7 ========\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    40  of    245.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    245.    Elapsed: 0:00:18.\n",
            "  Batch   120  of    245.    Elapsed: 0:00:27.\n",
            "  Batch   160  of    245.    Elapsed: 0:00:35.\n",
            "  Batch   200  of    245.    Elapsed: 0:00:44.\n",
            "  Batch   240  of    245.    Elapsed: 0:00:53.\n",
            "\n",
            "  Training loss: 0.78\n",
            "  Training epoch took: 0:00:54\n",
            "  Accuracy: 70.968 %\n",
            "  Test Loss: 0.679\n",
            "  Confusion Matrix: \n",
            "[[56 19  0  0  0]\n",
            " [21 57  0  6  2]\n",
            " [ 1  3  0  1  5]\n",
            " [ 2  7  0 53  0]\n",
            " [ 0  0  0  2 10]]\n",
            "  Precision Score: 56.117 %\n",
            "  Recall Score: 61.953 %\n",
            "  F1 Score: 58.597 %\n",
            "\n",
            "======== Epoch 3 / 7 ========\n",
            "Training:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch    40  of    245.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    245.    Elapsed: 0:00:18.\n",
            "  Batch   120  of    245.    Elapsed: 0:00:27.\n",
            "  Batch   160  of    245.    Elapsed: 0:00:35.\n",
            "  Batch   200  of    245.    Elapsed: 0:00:44.\n",
            "  Batch   240  of    245.    Elapsed: 0:00:53.\n",
            "\n",
            "  Training loss: 0.60\n",
            "  Training epoch took: 0:00:54\n",
            "  Accuracy: 69.355 %\n",
            "  Test Loss: 0.953\n",
            "  Confusion Matrix: \n",
            "[[49 25  1  0  0]\n",
            " [14 58  2 10  2]\n",
            " [ 0  2  1  1  6]\n",
            " [ 2  5  0 55  0]\n",
            " [ 0  0  1  2  9]]\n",
            "  Precision Score: 58.731 %\n",
            "  Recall Score: 61.297 %\n",
            "  F1 Score: 59.185 %\n",
            "\n",
            "======== Epoch 4 / 7 ========\n",
            "Training:\n",
            "  Batch    40  of    245.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    245.    Elapsed: 0:00:18.\n",
            "  Batch   120  of    245.    Elapsed: 0:00:27.\n",
            "  Batch   160  of    245.    Elapsed: 0:00:35.\n",
            "  Batch   200  of    245.    Elapsed: 0:00:44.\n",
            "  Batch   240  of    245.    Elapsed: 0:00:53.\n",
            "\n",
            "  Training loss: 0.37\n",
            "  Training epoch took: 0:00:54\n",
            "  Accuracy: 70.161 %\n",
            "  Test Loss: 1.500\n",
            "  Confusion Matrix: \n",
            "[[55 20  0  0  0]\n",
            " [19 61  0  5  1]\n",
            " [ 1  4  2  3  0]\n",
            " [ 2 10  0 50  0]\n",
            " [ 0  1  0  5  6]]\n",
            "  Precision Score: 80.010 %\n",
            "  Recall Score: 58.982 %\n",
            "  F1 Score: 63.179 %\n",
            "\n",
            "======== Epoch 5 / 7 ========\n",
            "Training:\n",
            "  Batch    40  of    245.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    245.    Elapsed: 0:00:18.\n",
            "  Batch   120  of    245.    Elapsed: 0:00:26.\n",
            "  Batch   160  of    245.    Elapsed: 0:00:35.\n",
            "  Batch   200  of    245.    Elapsed: 0:00:44.\n",
            "  Batch   240  of    245.    Elapsed: 0:00:53.\n",
            "\n",
            "  Training loss: 0.19\n",
            "  Training epoch took: 0:00:54\n",
            "  Accuracy: 77.419 %\n",
            "  Test Loss: 1.520\n",
            "  Confusion Matrix: \n",
            "[[61 13  1  0  0]\n",
            " [22 56  2  5  1]\n",
            " [ 0  2  7  0  1]\n",
            " [ 2  6  0 54  0]\n",
            " [ 0  0  0  1 11]]\n",
            "  Precision Score: 77.821 %\n",
            "  Recall Score: 79.043 %\n",
            "  F1 Score: 78.297 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Target Acquired. Stopping training process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}